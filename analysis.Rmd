
We first read the data into R, remove any missing client_ids (there shouldn't be
any), convert some of the variables into factors 

```{r readSource,cache=TRUE}
library(data.table)
s <- fread("~/tmp/sentiment56.csv")
s <- s[cid!="",]
invisible({s[, ":="(
               mem=factor(mem,ordered=TRUE),
               tcrash= pmax(tcrash,0),
               branch=factor(branch,levels=c("satisfied-1","like-1","recommend-1",
                                             "keep-using-1","up-to-expectations-1",
                                             "favorite-1")),
               question=factor(question,levels=c("Are you satisfied with Firefox?",
                                                 "Do you like Firefox?",
                                                 "Would you recommend Firefox to a friend or family member?",
                                                 "Will you keep using Firefox in the future?",
                                                 "Is Firefox performing up to your expectations?",
                                                 "Is Firefox is your favorite browser?")),
               responder=factor(responder,levels=c('nr','r')),
               response=factor(response, ordered=TRUE),
               os=factor(os, levels=c('Windows_NT','Darwin','Linux')),
               synq = factor(synq, ordered=TRUE),
               isdefault=factor(isdefault, ordered=TRUE),
               country2=sapply(country, function(s) if( s %in% c("US","IN","GB","ID","CA","AU")) s else "other"),
               addonsModified=factor(addonsModified))]
})
attr (s$mem, "contrasts") <- contr.poly (4) 
```

Though not visible in the above CSV file, **not all** subjects in the experiment
could be found in `main-summary` or `clients_daily`. In fact we couldn't find 

```{r nmissing, cache=TRUE}
nmissing <- fread("~/tmp/sentiment56_missing.csv")[, length(unique(client_id))]
```

`r nmissing` profiles in our main data sets. *How can this happen?*

What is our response rate?

```{r}
s[, length(unique(cid)),by=responder]
```

Response rate by day


```{r ResponseRateByDay}
x <- s[, length(unique(cid)),by=list(dateWhenQuestioned,responder)]
x <- reshape(x,dir='wide',v.names='V1',timevar='responder',
             idvar='dateWhenQuestioned')[order(dateWhenQuestioned),]
setnames(x,c("date","nonresponse","response"))
invisible(x[, ResponseRate:=response/(response+nonresponse)*100])
datatable(x)
```

```{r}
xyplot(ResponseRate~ as.Date(date), type=c('l','g'), scale=list(tick.num=10), lwd=2,xlab='Date When Questioned',
       ylab='Response Rate',data=x)
```

How many profiles were created during the course of the survey?

```{r}
s[, sum(is.na(ageAtBeginningofExp))+sum(!is.na(ageAtBeginningofExp) & ageAtBeginningofExp<=0)]
```


Sample data

```{r}
datatable(head(s,10),options = list(dom = 't',scrollX = TRUE))
```

# What Affects Response Rates?

How many profiles assigned to each question

```{r}
invisible(s[,i:=1])
t(xtabs( i ~ responder+question, data=s))
```
and the response rate to each question

```{r}
x <- data.table(prop.table(t(xtabs( i ~ responder+question, data=s)),1))
x <- reshape(x,dir='wide',v.names='N',timevar='responder',idvar='question')[order(N.r),]
datatable(x)
```

What is the likelihood of responding? Is it affected by them using the browser
more than those that don't? New profiles vs old profiles? Variables we'll
consider

Keep in mind the questions are assigned randomly to profiles. Hence the
distribution of the covariates (e.g. distribution of new profiles) will be the
same across all questions. It wont be the case that we have more of one
kind in one question vs another *but* a covariate might have different association with
the response distribution in a question.

For example consider country,

```{r}
s[,prop.table( xtabs(I( responder =='nr') ~ country2))*100]
s[,prop.table( xtabs(I( responder =='r') ~ country2))*100]
```

There is a slight difference (e.g profiles in India are liklier to respond)
and consquently there is a slight country asymmetry in responders and non
responders.

We will consider the following covariates

-  **age of profile**, are newer profiles more likely to respond (new is
   profiles less than 10 days old when they were asked the question)?
- **os** do profiles on a particular platform more disposed to answering stuff?
- **total hours**, is a user who uses Firefox more, more likely to respond?
- **total usage per day*** if they use it more often does it make them more likely
  to respond  
- **did they crash** does a recent history of crashing affect response rate?
- **time since last crash**, does a crash closer in time affect response

Also we will clean the data of outliers.

```{r quantiles}
require(DT)
P <- c(0:9/10,seq(0.95,1,length=21))
datatable({
    x <- rbind(data.table(p=P,var='ah',x=s[, quantile(ah,P)])
              ,data.table(p=P,var='th',x=s[, quantile(th,P)]))
    reshape(x,dir='wide',v.names='x',timevar='p',idvar='var')
},options = list(dom = 't',scrollX = TRUE))
```

Based on the above we'll cap some of the data (rather than drop the rows)

```{r capping1}
invisible({
    s2 <- s
    s2$id <- 1
    s2[, ":="(ah=pmin(24*29,pmin(ah,quantile(ah,0.999))),
              th=pmin(24*29,pmin(th,quantile(th,0.999))),
              turi=pmin(turi,quantile(turi,0.999)),
              ttabwin=pmin(ttabwin, quantile(ttabwin, 0.999))
              )]
    s2 <- s2[,":="(
        newprofile = ageAtBeginningofExp <=10,
        ahDay = pmin(24,ah/ndays),
        thDay = pmin(24,th/ndays),
        uriDay = turi/ndays,
        ttabDay=ttabwin/ndays,
        dcrash=sapply(daysSinceLastCrashed, function(s)if(is.na(s)) 29 else s)
    )][,]
})
```

Consider then a model that examines if any covariate explains differences in
response rate? Rather than fit one model, we will fit separate models to the
questions.


```{r responseModel}

mb <- s2[,{
    modelSimp <- glm( responder ~ newprofile + os + country2+th+uriDay+ttabDay+ ndays+I(log(1+tcrash/(th+1/60)))  , family='binomial' ,  data=.SD)
    x <- .SD
    x$ph <- predict(modelSimp,type='response')
    x2 <- x[,mean(responder=='r'), by=list(z=cut(ph,quantile(ph,0:5/5),include.lowest=TRUE))][order(z),]
    require(InformationValue)
    prec <- precision(x$responder=='r', x$ph)
    sens <- sensitivity(x$responder=='r',x$ph)
    concord <- list(Concordance=1) #Concordance(x$responder=='r',x$ph)
    plot <- ks_plot(x$responder=='r',x$ph)
    list(m=list(modelSimp),r=mean(responder=='r'), x=list(x2), prec=prec,sens=sens,con=concord$Concordance)
},by=branch]

```

## see counry=India response, crashes dont affect response, it's all usage hors


In the model below, we see  little to no interaction between branches and covariates

```{r responseRateModel,cache=TRUE}
invisible({

    model <- glm( factor(responder) ~ branch*(newprofile + os +
        log(th+1)+log(thDay+1) + didCrash), family='binomial',
        data=s2)

    s3 <- s2[question=='Are you satisfied with Firefox?',]
})



    s3 <- s2[question=='Will you keep using Firefox in the future?',]
    modelSimp <- glm( factor(responder) ~ newprofile + os + th+thDay , family='binomial', data=s3)
s3$ph <- predict(modelSimp,type='response')
    s3$od <- predict(modelSimp,type='link')
    s3[,mean(responder=='r'), by=list(branch,z=cut(ph,quantile(ph,0:5/5),include.lowest=TRUE))][order(branch, z),]
    s3[, mean(responder=='r')]


s2 <- s
s2$id <- 1
s2[, ":="(
    newprofile = ageAtBeginningofExp<=10,
    betaCountry = country %in% c("ID","IN","PL","BR","US","DE"),
    ah=pmin(24*28,pmin(ah,quantile(ah,0.999))),
    th=pmin(24*28,pmin(th,quantile(th,0.999))),
    turi=pmin(turi,quantile(turi,0.999)),
    ttabwin=pmin(ttabwin, quantile(ttabwin, 0.999)),
    tsrch  =pmin(tsrch,quantile(tsrch,0.999))
)]
s2[,":="(    ahDay = pmin(24,ah/ndays),
         thDay = pmin(24,th/ndays))]


s2[, ":="(
    thf = cut(th, quantile(th, 0:10/10),include.lowest=TRUE,ordered=TRUE),
    tuf = cut(turi, quantile(tsrch,0:5/5),include.lowest=TRUE,ordered=TRUE),
    ttabf= cut(ttabwin,quantile(ttabwin,0:10/10),include.lowest=TRUE,ordered=TRUE),
    tsf = cut(tsrch, quantile(tsrch,0:5/5),include.lowest=TRUE,ordered=TRUE)
)]

xm <- glm( responder ~ thDay + th + tsrch + newprofile + os+didCrash,data=s2,family='binomial')


```

    model2 <- glm( factor(responder) ~ (newprofile + os +
    ndays+sqrt(thDay) + sqrt(uriDay)+sqrt(ttabDay+1)
    ), family='binomial', data=s2)
    
model2 <- glm( factor(responder) ~ newprofile + os + pc1+pc2+pc3    , family='binomial', data=s2)
```{r}
summary(model)
```

so we finally use a non interaction model to explain what drives response rate
with the model summary followed by a plot of the confidence intervals of the
coefficients. 

```{r}
summary(modelSimp)
```

```{r ciModelSimple, cache=TRUE,dependson="responseRateModel"}
library(CIplot) 
CIplot(modelSimp)
```

```{r bootLogInterval}
bi <- function(fml, data, coefName,rep=500,G=exp){
    require(boot)
    myf <- function(da,i){
        d <- da[i,]
        uu <- glm( as.formula(fml), family='binomial', data=d)
       G(coef(uu)[coefName])
    }
    b <- boot(data, myf, R=rep)
    boot.ci(b, type='norm')$normal[-1]
}
```

```{r confCoeefModel1,cache=TRUE}
fml <- responder ~ (newprofile + os + log(th+1)+log(thDay+1 ) + didCrash)
newprofile <- bi(fml=fml, data=s2, coefName='newprofileTRUE',rep=100)
thday <- bi(fml=fml, data=s2, coefName='log(thDay + 1)',
            rep=100,G=function(x) x)
```

We can describe this as

- being a new profile (everything else the same) increases the odds of
  responding by factor between   `r newprofile`.
- windows users are least likely to respond but they still contribute
  most responses (by sheer volume). But this does mean that the
  response distribution is skewed slightly less than usual. For
  example consider the overall distribution of os  compared to the
  responder distribution. The difference is unlikely to change much of
  our decisions.

```{r winbias}
s2[, prop.table(table(os))]
s2[responder=='r', prop.table(table(os))]
```

- the more hours one uses (both amount in a 28 day period and per day use) are
  associated with more use . Interestingly had i used *number of days used*, its
  coefficient would be negative. This is because both the coefficients of total
  hours and hours per day is positive. I would like to point out this is not
  "more you use the browser the likelier you're to see the prompt", because both
  responder and non responders saw the question
- you'll see, surprisingly, a crashier profile is likelier to respond. However
  this is not the case: those with crashes, have many hours of use and that is
  what drives the response.  

## Principal Components to Incorporate Usage

We have other measures to and we can include them separately. 
